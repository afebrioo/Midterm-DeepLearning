{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e7f43a",
   "metadata": {},
   "source": [
    "Rahmanda Afebrio Yuris Soesatyo - 1103223024 / Midterm-Deep Learning-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a756ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1f8eaAZY-7YgFxLcrL3OkvSRa3onNNLb9\n",
      "From (redirected): https://drive.google.com/uc?id=1f8eaAZY-7YgFxLcrL3OkvSRa3onNNLb9&confirm=t&uuid=eba977a2-eafa-4455-ac63-69c2316cd777\n",
      "To: c:\\College\\Deep Learning\\Midterm\\midterm-regresi-dataset.csv\n",
      "100%|██████████| 443M/443M [00:58<00:00, 7.61MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'midterm-regresi-dataset.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download(\"https://drive.google.com/uc?id=1f8eaAZY-7YgFxLcrL3OkvSRa3onNNLb9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d78a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2001</th>\n",
       "      <th>49.94357</th>\n",
       "      <th>21.47114</th>\n",
       "      <th>73.0775</th>\n",
       "      <th>8.74861</th>\n",
       "      <th>-17.40628</th>\n",
       "      <th>-13.09905</th>\n",
       "      <th>-25.01202</th>\n",
       "      <th>-12.23257</th>\n",
       "      <th>7.83089</th>\n",
       "      <th>...</th>\n",
       "      <th>13.0162</th>\n",
       "      <th>-54.40548</th>\n",
       "      <th>58.99367</th>\n",
       "      <th>15.37344</th>\n",
       "      <th>1.11144</th>\n",
       "      <th>-23.08793</th>\n",
       "      <th>68.40795</th>\n",
       "      <th>-1.82223</th>\n",
       "      <th>-27.46348</th>\n",
       "      <th>2.26327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.54767</td>\n",
       "      <td>0.31568</td>\n",
       "      <td>92.35066</td>\n",
       "      <td>22.38696</td>\n",
       "      <td>-25.51870</td>\n",
       "      <td>-19.04928</td>\n",
       "      <td>20.67345</td>\n",
       "      <td>-5.19943</td>\n",
       "      <td>3.63566</td>\n",
       "      <td>...</td>\n",
       "      <td>6.59753</td>\n",
       "      <td>-50.69577</td>\n",
       "      <td>26.02574</td>\n",
       "      <td>18.94430</td>\n",
       "      <td>-0.33730</td>\n",
       "      <td>6.09352</td>\n",
       "      <td>35.18381</td>\n",
       "      <td>5.00283</td>\n",
       "      <td>-11.02257</td>\n",
       "      <td>0.02263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2001  49.94357  21.47114   73.0775   8.74861  -17.40628  -13.09905  \\\n",
       "0  2001  48.73215  18.42930  70.32679  12.94636  -10.32437  -24.83777   \n",
       "1  2001  50.95714  31.85602  55.81851  13.41693   -6.57898  -18.54940   \n",
       "2  2001  48.24750  -1.89837  36.29772   2.58776    0.97170  -26.21683   \n",
       "3  2001  50.97020  42.20998  67.09964   8.46791  -15.85279  -16.81409   \n",
       "4  2001  50.54767   0.31568  92.35066  22.38696  -25.51870  -19.04928   \n",
       "\n",
       "   -25.01202  -12.23257   7.83089  ...   13.0162  -54.40548  58.99367  \\\n",
       "0    8.76630   -0.92019  18.76548  ...   5.66812  -19.68073  33.04964   \n",
       "1   -3.27872   -2.35035  16.07017  ...   3.03800   26.05866 -50.92779   \n",
       "2    5.05097  -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705   \n",
       "3  -12.48207   -9.37636  12.63699  ...   9.92661  -55.95724  64.92712   \n",
       "4   20.67345   -5.19943   3.63566  ...   6.59753  -50.69577  26.02574   \n",
       "\n",
       "   15.37344   1.11144  -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "0  42.87836  -9.90378  -32.22788   70.49388  12.04941   58.43453  26.92061  \n",
       "1  10.93792  -0.07568   43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n",
       "2 -46.67617 -12.51516   82.58061  -72.08993   9.90558  199.62971  18.85382  \n",
       "3 -17.72522  -1.49237   -7.50035   51.76631   7.88713   55.66926  28.74903  \n",
       "4  18.94430  -0.33730    6.09352   35.18381   5.00283  -11.02257   0.02263  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = 'midterm-regresi-dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e8167",
   "metadata": {},
   "source": [
    "Terlihat bahwasannya dataset tidak memiliki header, lanjut ke preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (515344, 91)\n"
     ]
    }
   ],
   "source": [
    "print(\"df.shape:\", df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416f1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TensorFlow / Keras Imports\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TensorFlow warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Scikit-learn Imports for Preprocessing and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb7d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total shape: (515345, 91)\n"
     ]
    }
   ],
   "source": [
    "# Since the dataset has no header, we manually create column names.\n",
    "feature_cols = [f'feature_{i}' for i in range(1, 91)]\n",
    "column_names = ['target'] + feature_cols\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Dataset loaded. Total shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5914c5",
   "metadata": {},
   "source": [
    "Testing split data train beserta standarisasi target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c87e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Using the full dataset (SAMPLE_FRACTION = 1.0).\n",
      "Sampled Dataset Shape: (515345, 91)\n",
      "Target variable (Year) standardized. Mean: 1998.40, Std Dev: 10.93\n",
      "Training set size: 412276 rows\n",
      "Test set size: 103069 rows\n"
     ]
    }
   ],
   "source": [
    "# Use a sample for demonstration. Change SAMPLE_FRACTION to 1.0 to use the full dataset.\n",
    "SAMPLE_FRACTION = 1\n",
    "\n",
    "if SAMPLE_FRACTION < 1.0:\n",
    "    print(f\"Status: Using a sample. Sampling {SAMPLE_FRACTION*100:.0f}% of the data.\")\n",
    "    df_sample = df.sample(frac=SAMPLE_FRACTION, random_state=42).copy()\n",
    "else:\n",
    "    print(\"Status: Using the full dataset (SAMPLE_FRACTION = 1.0).\")\n",
    "    df_sample = df.copy()\n",
    "\n",
    "print(f\"Sampled Dataset Shape: {df_sample.shape}\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_sample.drop('target', axis=1)\n",
    "y = df_sample['target'].copy()\n",
    "numerical_features = X.columns.tolist()\n",
    "\n",
    "# The target variable (year, e.g., 2001) is often large.\n",
    "# Standardizing the target helps Deep Learning models converge faster.\n",
    "target_mean = y.mean()\n",
    "target_std = y.std()\n",
    "\n",
    "# Target Standardization: y_scaled = (y - mean) / std\n",
    "y_scaled = (y - target_mean) / target_std\n",
    "print(f\"Target variable (Year) standardized. Mean: {target_mean:.2f}, Std Dev: {target_std:.2f}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train_scaled, y_test_scaled = train_test_split(\n",
    "    X, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "y_test_original = y.loc[y_test_scaled.index] # Keep original years for final reporting\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "print(f\"Test set size: {X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19df7fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features processed. Training shape: (412276, 90)\n"
     ]
    }
   ],
   "source": [
    "# We only need scaling for the features, as the target is handled above.\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the scaling to all 90 features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Apply preprocessing and convert to NumPy array for TensorFlow\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Features processed. Training shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ac49ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. TensorFlow Model Definition (MLP) ---\n",
      "Input Features: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\College\\Deep Learning\\Midterm\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m11,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,017</span> (86.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,017\u001b[0m (86.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,017</span> (86.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,017\u001b[0m (86.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. TensorFlow Model Definition (MLP) ---\n",
    "print(\"\\n--- 4. TensorFlow Model Definition (MLP) ---\")\n",
    "\n",
    "input_shape = X_train_processed.shape[1]\n",
    "print(f\"Input Features: {input_shape}\")\n",
    "\n",
    "# Simple Multi-Layer Perceptron (MLP) for Regression\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential([\n",
    "        # Input Layer: 90 features\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2), # Dropout for regularization\n",
    "        \n",
    "        # Hidden Layer 1\n",
    "        Dense(64, activation='relu'),\n",
    "        \n",
    "        # Hidden Layer 2\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        # Output Layer: 1 unit for the scaled target value\n",
    "        Dense(1, activation='linear') \n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    # Using 'Adam' optimizer and 'mse' loss for regression on scaled target\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "model = create_mlp_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7747fd",
   "metadata": {},
   "source": [
    "Training model dengan early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d52b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5798/5798 - 7s - 1ms/step - loss: 0.6994 - mae: 0.5850 - mse: 0.6994 - val_loss: 0.6473 - val_mae: 0.5495 - val_mse: 0.6473\n",
      "Epoch 2/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6523 - mae: 0.5594 - mse: 0.6523 - val_loss: 0.6295 - val_mae: 0.5453 - val_mse: 0.6295\n",
      "Epoch 3/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6420 - mae: 0.5549 - mse: 0.6420 - val_loss: 0.6271 - val_mae: 0.5435 - val_mse: 0.6271\n",
      "Epoch 4/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6335 - mae: 0.5505 - mse: 0.6335 - val_loss: 0.6238 - val_mae: 0.5380 - val_mse: 0.6238\n",
      "Epoch 5/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6274 - mae: 0.5473 - mse: 0.6274 - val_loss: 0.6188 - val_mae: 0.5425 - val_mse: 0.6188\n",
      "Epoch 6/100\n",
      "5798/5798 - 7s - 1ms/step - loss: 0.6230 - mae: 0.5457 - mse: 0.6230 - val_loss: 0.6196 - val_mae: 0.5474 - val_mse: 0.6196\n",
      "Epoch 7/100\n",
      "5798/5798 - 7s - 1ms/step - loss: 0.6185 - mae: 0.5436 - mse: 0.6185 - val_loss: 0.6151 - val_mae: 0.5353 - val_mse: 0.6151\n",
      "Epoch 8/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6143 - mae: 0.5418 - mse: 0.6143 - val_loss: 0.6101 - val_mae: 0.5394 - val_mse: 0.6101\n",
      "Epoch 9/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6098 - mae: 0.5399 - mse: 0.6098 - val_loss: 0.6149 - val_mae: 0.5477 - val_mse: 0.6149\n",
      "Epoch 10/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.6074 - mae: 0.5387 - mse: 0.6074 - val_loss: 0.6119 - val_mae: 0.5411 - val_mse: 0.6119\n",
      "Epoch 11/100\n",
      "5798/5798 - 7s - 1ms/step - loss: 0.6048 - mae: 0.5376 - mse: 0.6048 - val_loss: 0.6088 - val_mae: 0.5368 - val_mse: 0.6088\n",
      "Epoch 12/100\n",
      "5798/5798 - 7s - 1ms/step - loss: 0.6015 - mae: 0.5359 - mse: 0.6015 - val_loss: 0.6097 - val_mae: 0.5428 - val_mse: 0.6097\n",
      "Epoch 13/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5996 - mae: 0.5356 - mse: 0.5996 - val_loss: 0.6094 - val_mae: 0.5375 - val_mse: 0.6094\n",
      "Epoch 14/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5974 - mae: 0.5345 - mse: 0.5974 - val_loss: 0.6092 - val_mae: 0.5400 - val_mse: 0.6092\n",
      "Epoch 15/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5959 - mae: 0.5336 - mse: 0.5959 - val_loss: 0.6069 - val_mae: 0.5342 - val_mse: 0.6069\n",
      "Epoch 16/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5925 - mae: 0.5327 - mse: 0.5925 - val_loss: 0.6058 - val_mae: 0.5382 - val_mse: 0.6058\n",
      "Epoch 17/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5916 - mae: 0.5322 - mse: 0.5916 - val_loss: 0.6098 - val_mae: 0.5400 - val_mse: 0.6098\n",
      "Epoch 18/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5884 - mae: 0.5309 - mse: 0.5884 - val_loss: 0.6109 - val_mae: 0.5352 - val_mse: 0.6109\n",
      "Epoch 19/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5864 - mae: 0.5301 - mse: 0.5864 - val_loss: 0.6082 - val_mae: 0.5380 - val_mse: 0.6082\n",
      "Epoch 20/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5859 - mae: 0.5298 - mse: 0.5859 - val_loss: 0.6036 - val_mae: 0.5348 - val_mse: 0.6036\n",
      "Epoch 21/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5840 - mae: 0.5293 - mse: 0.5840 - val_loss: 0.6068 - val_mae: 0.5348 - val_mse: 0.6068\n",
      "Epoch 22/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5827 - mae: 0.5286 - mse: 0.5827 - val_loss: 0.6068 - val_mae: 0.5326 - val_mse: 0.6068\n",
      "Epoch 23/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5810 - mae: 0.5280 - mse: 0.5810 - val_loss: 0.6099 - val_mae: 0.5338 - val_mse: 0.6099\n",
      "Epoch 24/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5800 - mae: 0.5277 - mse: 0.5800 - val_loss: 0.6059 - val_mae: 0.5349 - val_mse: 0.6059\n",
      "Epoch 25/100\n",
      "5798/5798 - 6s - 1ms/step - loss: 0.5786 - mae: 0.5269 - mse: 0.5786 - val_loss: 0.6058 - val_mae: 0.5346 - val_mse: 0.6058\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Training Duration: 158.89 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "EPOCHS = 100 \n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1, # Use 10% of training data for validation\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2 # Verbose 2 shows progress per epoch\n",
    ")\n",
    "training_duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining Duration: {training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9a994",
   "metadata": {},
   "source": [
    "Evaluasi pada test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3a09e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 73.0717\n",
      "Root Mean Squared Error (RMSE): 8.5482\n",
      "Mean Absolute Error (MAE): 5.8431\n",
      "R-squared (R²): 0.3860\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the scaled test set\n",
    "y_pred_scaled = model.predict(X_test_processed, verbose=0).flatten()\n",
    "\n",
    "y_pred_denorm = (y_pred_scaled * target_std) + target_mean\n",
    "\n",
    "# Calculate evaluation metrics using the DENORMALIZED predictions and original labels\n",
    "mse = mean_squared_error(y_test_original, y_pred_denorm)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_original, y_pred_denorm)\n",
    "r2 = r2_score(y_test_original, y_pred_denorm)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
